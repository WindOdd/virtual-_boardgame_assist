from google import genai
from google.genai import types
from enum import Enum
def try_cloud_LLM(user_question:str):
    STORE_CLERK_INSTRUCTION = (
    "ä½ æ˜¯ä¸€ä½å°ˆæ¥­ã€é«˜æ•ˆçš„æ¡ŒéŠåº—ã€æ¨‚ç©åŠã€åº—å“¡ã€‚ä½ çš„æ ¸å¿ƒä»»å‹™æ˜¯é‡å°é¡§å®¢çš„æå•ï¼Œæä¾›ã€æœ€ç›´æ¥ã€æœ€ç²¾æº–ã€çš„éŠæˆ²è³‡è¨Šã€æ¨è–¦æˆ–å»ºè­°ã€‚æ‰€æœ‰å›ç­”éƒ½å°‡è½‰æˆèªéŸ³è¼¸å‡ºï¼Œå› æ­¤å¿…é ˆï¼š\
    1. æ¸›å°‘å¯’æš„ã€å•å€™èªã€‚\
    2. ç›´æ¥é€²å…¥ä¸»é¡Œï¼Œæä¾›ç­”æ¡ˆã€‚\
    3. èªæ°£ä¿æŒå°ˆæ¥­å‹å–„ï¼Œä½¿ç”¨å£èªåŒ–ç¹é«”ä¸­æ–‡ã€‚\
    4. é¿å…ä½¿ç”¨æ¸…å–®ã€è¡¨æƒ…ç¬¦è™Ÿï¼Œä¿æŒå›ç­”å…§å®¹ç°¡çŸ­ã€è‡ªç„¶ã€æµæš¢ã€‚\
    5.ä¸çŸ¥é“çš„å•é¡Œå°±ç›´æ¥å›ç­”ä¸çŸ¥é“\
    6.ä½ ç›®å‰åªè² è²¬éŠæˆ²ï¼šå¡å¡é Œ\
    7.å¦‚æœä½¿ç”¨è€…è©¢å•ä¸ç›¸å¹²çš„å•é¡Œå°±å›ç­”è«‹ä»–æ‰¾å…¶ä»–åº—å“¡")
    model_type="gemini-2.5-flash"
    # response = client.models.generate_content(
    # model=model_type,
    # contents=types.Part.from_text(text=user_question),
    # config=types.GenerateContentConfig(
    #     system_instruction=STORE_CLERK_INSTRUCTION,
    #     temperature=0.3,))
    # print(response.text)
    try:
        # ä¿®æ­£ 2: å¾ç’°å¢ƒè®Šæ•¸è®€å– Keyï¼Œæˆ–åœ¨æ­¤è™•æš«æ™‚è²¼ä¸Š(ä½†ä¸å»ºè­°æäº¤)
        #api_key = os.environ.get("GEMINI_API_KEY", "ä½ çš„_NEW_API_KEY")
        if not api_key or "ä½ çš„" in api_key:
             print("âŒ éŒ¯èª¤ï¼šè«‹è¨­å®šæ­£ç¢ºçš„ Gemini API Key")
             return None

        client = genai.Client(api_key=api_key)
        
        # ä½¿ç”¨ä¸²æµ (Stream) å¯ä»¥è®“ä½¿ç”¨è€…æ„Ÿè¦ºåæ‡‰è¼ƒå¿«
        print("ğŸ¤– Gemini æ€è€ƒä¸­...", end="", flush=True)
        gemini_config=types.GenerateContentConfig(
            system_instruction=STORE_CLERK_INSTRUCTION,
            temperature=0.3,
            top_p=0.9,
            top_k=20,
            max_output_tokens= 300,
        )
        full_response = ""
        response_stream = client.models.generate_content_stream(
            model=model_type,
            contents=types.Part.from_text(text=user_question),
            config=gemini_config
        )
        
        print("\nè™›æ“¬åº—å“¡: ", end="")
        for chunk in response_stream:
            if chunk.text:
                print(chunk.text, end='', flush=True)
                full_response += chunk.text
        
        print("\n") # æ›è¡Œ
        
        # client.close() # æ–°ç‰ˆ SDK é€šå¸¸ä¸éœ€è¦é¡¯å¼ closeï¼Œé™¤éæœ‰ç‰¹å®šéœ€æ±‚
        
        # ä¿®æ­£ 3: å›å‚³å®Œæ•´æ–‡å­—ï¼Œä»¥ä¾¿ä¸»ç¨‹å¼å‚³çµ¦ TTS (æ–‡å­—è½‰èªéŸ³)
        return full_response

    except Exception as e:
        print(f"\nâŒ Gemini API éŒ¯èª¤: {e}")
        return None
if __name__=="__main__":
    try_cloud_LLM("å¡å¡é Œçš„åŸºæœ¬è¦å‰‡æœ‰å“ªäº›")