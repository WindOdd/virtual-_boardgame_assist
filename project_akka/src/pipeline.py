"""
Project Akka - Pipeline Module
Orchestrator for Hybrid Routing (Semantic -> LLM)
Refactored for Stateless Architecture (v9.6)
"""

import logging
import asyncio
import random
from pathlib import Path
from typing import Optional, List, Dict, Any
from dataclasses import dataclass

try:
    from .llm import LLMServiceManager
except ImportError:
    from llm.manager import LLMServiceManager

# Project modules
try:
    from .boardgame_utils import ConfigLoader, PromptManager
    from .data_manager import get_data_manager
    from .semantic_router import SemanticRouter
except ImportError:
    from boardgame_utils import ConfigLoader, PromptManager
    from data_manager import get_data_manager
    from semantic_router import SemanticRouter

logger = logging.getLogger(__name__)

@dataclass
class RouterResult:
    intent: str
    confidence: float = 0.0
    source: str = "unknown"

@dataclass
class PipelineResult:
    response: str
    intent: Optional[str] = None
    confidence: float = 0.0
    source: str = "unknown"

class Pipeline:
    def __init__(self, config_dir: Optional[Path] = None):
        self.config_dir = config_dir or Path(__file__).parent.parent / "config"
        self.data_manager = get_data_manager()
        self.system_config = {}
        self.semantic_routes = {}
        
        # 1. Load Configurations
        self._load_configs()
        
        # 2. Initialize Semantic Router
        embedding_config = self.system_config.get("model", {}).get("embedding", {})
        self.semantic_router = SemanticRouter(
            model_config=embedding_config,
            routes_config=self.semantic_routes
        )
        
        # 3. Initialize LLM Manager
        self.llm_manager = LLMServiceManager(self.system_config)
        self.local_llm = self.llm_manager.get_local()
        self.cloud_llm = self.llm_manager.get_cloud()

    def _load_configs(self) -> None:
        """Load all YAML configurations."""
        try:
            self.store_info = ConfigLoader(self.config_dir / "store_info.yaml").load()
            self.intent_map = ConfigLoader(self.config_dir / "intent_map.yaml").load()
            self.local_prompts = PromptManager(self.config_dir / "prompts_local.yaml")
            self.cloud_prompts = PromptManager(self.config_dir / "prompts_cloud.yaml")
            self.system_config = ConfigLoader(self.config_dir / "system_config.yaml").load()
            
            # Optional Configs
            try:
                self.semantic_routes = ConfigLoader(self.config_dir / "semantic_routes.yaml").load()
            except Exception:
                logger.warning("semantic_routes.yaml missing.")
                self.semantic_routes = {}
            
            logger.info("Config loaded.")
        except Exception as e:
            logger.error(f"Config load failed: {e}")
            self.store_info = {}
            self.semantic_routes = {}
            self.intent_map = {}
            self.system_config = {}

    def reload_configs(self) -> None:
        logger.info("Reloading configurations...")
        self._load_configs()
        embedding_config = self.system_config.get("model", {}).get("embedding", {})
        self.semantic_router = SemanticRouter(embedding_config, self.semantic_routes)

    async def process(self, user_input: str, history: List[Dict[str, Any]] = None, llm_service=None) -> PipelineResult:
        """
        Main processing pipeline.
        Args:
            user_input: The current user query.
            history: List of past turns (provided by Client) to extract context.
        """
        user_input = user_input.strip()
        if not user_input:
            return PipelineResult(response="...", source="empty")

        # ============================================================
        # [NEW] Stage 0: Context Extraction (Stateless Logic)
        # ============================================================
        context_str = ""
        if history:
            # ç¯©é¸è¦å‰‡ï¼šåªçœ‹ User çš„ç™¼è¨€ï¼Œä¸”è©²ç™¼è¨€å¿…é ˆå¸¶æœ‰ intent
            recent_user_logs = [
                msg for msg in history 
                if msg.get("role") == "user" and msg.get("intent")
            ]
            
            if recent_user_logs:
                # å–å‡ºæœ€å¾Œ 2 æ¬¡çš„æ„åœ–è»Œè·¡ (ä¾‹å¦‚: RULES -> STORE_PRICING)
                last_intents = [msg["intent"] for msg in recent_user_logs[-2:]]
                context_str = " -> ".join(last_intents)
                logger.info(f"ğŸ•µï¸ Context Extracted from Request: {context_str}")

        # --- Stage 1: Semantic Vector Routing (FastPath) ---
        semantic_intent, score = self.semantic_router.route(user_input)
    
        if semantic_intent:
            logger.info(f"âš¡ FastPath Hit: {semantic_intent} (Score: {score:.4f})")
            response, source = await self._dispatch(semantic_intent, user_input, None)
            return PipelineResult(
                response=response,
                intent=semantic_intent,
                confidence=float(score),
                source=f"fastpath_{source}"
            )

        # --- Stage 2: LLM Intent Routing ---
        if self.llm_manager is None:
            return PipelineResult(response="ç³»çµ±ç¶­è­·ä¸­...", source="error")

        logger.info("ğŸ¢ FastPath Miss. Engaging LLM Router...")
    
        # [MODIFY] å°‡ Context æ³¨å…¥ Prompt
        if context_str:
            final_input = f"[Context: {context_str}] User Input: {user_input}"
        else:
            final_input = user_input

        # å‚³é€ final_input çµ¦ Router
        router_result = await self._route_with_llm(final_input, self.local_llm)
        # ä½¿ç”¨ Logger è€Œä¸æ˜¯ Printï¼Œä¿æŒ Log ä¹¾æ·¨
        logger.info(f"Router Result: {router_result}") 

        # --- Stage 3: Safety Filter ---
        if router_result.intent == "SENSITIVE":
            if self._check_allowlist(user_input):
                router_result.intent = "RULES"

        # --- Stage 4: Dispatch ---
        response, source = await self._dispatch(
            router_result.intent, 
            user_input, 
            llm_service
        )
        
        return PipelineResult(
            response=response,
            intent=router_result.intent,
            confidence=router_result.confidence,
            source=source
        )

    async def _route_with_llm(self, final_input: str, llm_service) -> RouterResult:
        """
        Sends the constructed input (with context) to the Local LLM.
        """
        router_config = self.local_prompts.get_task_config("router")
        system_prompt = router_config.get("system_prompt", "")
        
        if not system_prompt:
            logger.warning("Router system prompt is empty!")
            
        if not self.local_llm:
            return RouterResult(intent="UNKNOWN", confidence=0.0, source="error")
            
        try:
            # ç›´æ¥ä½¿ç”¨å·²ç¶“çµ„å¥½çš„ final_input
            response = await self.local_llm.generate_json(final_input, system_prompt)
            intent = response.get("intent", "UNKNOWN")
            confidence = response.get("confidence", 0.0)
            return RouterResult(intent=intent, confidence=confidence, source="llm")
        except Exception as e:
            logger.error(f"Router LLM Error: {e}")
            return RouterResult(intent="UNKNOWN", confidence=0.0, source="fallback")

    def _check_allowlist(self, user_input: str) -> bool:
        try:
            games = self.data_manager.list_games()
            for game in games:
                allowlist = game.metadata.get("allowlist_keywords", [])
                for keyword in allowlist:
                    if keyword in user_input: return True
        except Exception:
            pass
        return False

    async def _dispatch(self, intent: str, user_input: str, context: Any = None) -> tuple[str, str]:
        responses_map = self.store_info.get("responses", {})
        
        # 1. Static Responses
        if intent in responses_map:
            candidates = responses_map[intent]
            if candidates: return (random.choice(candidates), "content_static")
        
        # 2. Logic Handlers
        logic_intents = self.intent_map.get("logic_intents", {})
        if intent in logic_intents:
            logic_config = logic_intents[intent]
            handler = logic_config.get("handler", "")
            
            if handler == "local_llm" and self.local_llm:
                # é€™è£¡èª¿ç”¨ Persona é€²è¡Œé–’èŠ
                task = logic_config.get("task", "casual_chat")
                sys_prompt = self.local_prompts.get_task_config("casual_chat").get("system_prompt", "")
                resp = await self.local_llm.generate(user_input, sys_prompt)
                return (resp.content, "local_llm_gen")
            elif handler in ["online_llm", "cloud_llm", "cloud_rag"]:
                if self.cloud_llm:
                    return await self._handle_rules_query(user_input, context)
                else:
                    return ("æŠ±æ­‰ï¼Œé›²ç«¯å¤§è…¦é€£ç·šæœ‰é»å•é¡Œï¼Œè«‹ç¨å¾Œå†è©¦ã€‚", "error")
            elif handler == "reject":
                return (logic_config.get("response", "æŠ±æ­‰"), "reject")

        # 3. Fallback
        fallback = self.store_info.get("responses", {}).get("UNKNOWN_FALLBACK", ["æŠ±æ­‰ï¼Ÿ"])
        return (random.choice(fallback), "fallback")
    # src/pipeline.py -> class Pipeline

    async def _handle_rules_query(self, user_input: str, context: Dict[str, Any]) -> tuple[str, str]:
        """
        å°ˆé–€è™•ç† RULES æ„åœ–çš„é‚è¼¯å‡½å¼
        1. è§£æ Client å‚³ä¾†çš„ Game Name
        2. é€é DataManager è¼‰å…¥å°æ‡‰è¦å‰‡æ›¸
        3. çµ„åˆ Prompt ä¸¦å‘¼å« Cloud LLM
        """
        # 1. å–å¾—éŠæˆ²åç¨± (å¾ Client å‚³ä¾†çš„ context)
        game_ctx = context.get("game_context", {})
        # å„ªå…ˆä½¿ç”¨ Client å‚³ä¾†çš„ IDï¼Œå¦‚æœæ²’æœ‰å‰‡ç”¨é è¨­å€¼ (å¦‚ 'Carcassonne')
        game_id = game_ctx.get("game_name", "Carcassonne")
        
        # 2. é€é DataManager å–å¾—è¦å‰‡å…§å®¹ (å®ƒæœƒè‡ªå‹•æŸ¥ registry æ‰¾è·¯å¾‘)
        # é€™è£¡åˆ©ç”¨äº†æ‚¨ç¾æœ‰çš„ data_manager.py çš„åŠŸèƒ½
        rule_content = self.data_manager.get_rules(game_id)
        
        if not rule_content:
            logger.warning(f"Rulebook not found for game_id: {game_id}")
            # å˜—è©¦ç”¨é€šç”¨è¦å‰‡æˆ–å›å‚³éŒ¯èª¤
            rule_content = "ï¼ˆç³»çµ±æç¤ºï¼šç›®å‰æ‰¾ä¸åˆ°æ­¤éŠæˆ²çš„è©³ç´°è¦å‰‡è³‡æ–™ï¼Œè«‹ä¾æ“šæ‚¨çš„é€šç”¨çŸ¥è­˜å›ç­”ï¼Œä¸¦å‘ŠçŸ¥ä½¿ç”¨è€…è¦å‰‡æ›¸ç¼ºå¤±ã€‚ï¼‰"

        # 3. è®€å– System Prompt Template (å¾ prompts_cloud.yaml)
        # task_name å°æ‡‰ prompts_cloud.yaml è£¡çš„ key
        task_config = self.cloud_prompts.get_task_config("rules_explainer")
        system_template = task_config.get("system_prompt", "")
        
        # 4. æ³¨å…¥è¦å‰‡ (Prompt Injection)
        # å°‡ Template ä¸­çš„ {INJECTED_RAG_CONTENT} æ›¿æ›æˆçœŸå¯¦è¦å‰‡å…§å®¹
        final_system_prompt = system_template.replace("{INJECTED_RAG_CONTENT}", rule_content)
        
        # 5. æº–å‚™æ­·å²ç´€éŒ„ (History)
        history = context.get("history", [])
        
        # 6. å‘¼å«é›²ç«¯å¤§è…¦ (Cloud LLM)
        try:
            # å‘¼å« src/llm/cloud_llm_client.py
            response = await self.cloud_llm.generate(
                prompt=user_input,
                # æ³¨æ„ï¼šé€™è£¡æˆ‘å€‘æŠŠ "System Prompt + è¦å‰‡æ›¸" ç•¶ä½œ system_instruction å‚³å…¥
                system_prompt=final_system_prompt
                # History çš„è™•ç†é€šå¸¸ç”± LLM Client å…§éƒ¨è™•ç†ï¼Œæˆ–æ˜¯é€™è£¡å°‡ history è½‰æˆæ–‡å­—ä¸²æ¥åœ¨ prompt å‰
                # è¦–æ‚¨çš„ CloudLLMClient å¯¦ä½œè€Œå®šã€‚å¦‚æœæ˜¯ Google GenAI SDKï¼Œé€šå¸¸éœ€è¦è½‰æˆ message listã€‚
                # é€™è£¡å‡è¨­ generate æ–¹æ³•èƒ½è™•ç†åŸºç¤æ–‡å­—ç”Ÿæˆã€‚
            )
            return (response.content, "cloud_gen")
            
        except Exception as e:
            logger.error(f"Cloud Handling Error: {e}")
            return ("æŠ±æ­‰ï¼Œé›²ç«¯å¤§è…¦é€£ç·šæœ‰é»å•é¡Œï¼Œè«‹ç¨å¾Œå†è©¦ã€‚", "error")
def create_pipeline(config_dir: Optional[Path] = None) -> Pipeline:
    return Pipeline(config_dir=config_dir)

if __name__ == "__main__":
    # Test Standalone Pipeline
    import time 
    logging.basicConfig(level=logging.INFO)
    p = create_pipeline()
    print("Pipeline Initialized.")
    print("=============================")
    
    # æ¸¬è©¦ 1: ä¸€èˆ¬ FastPath
    print("\n--- Test 1: FastPath (No History) ---")
    print(asyncio.run(p.process("æƒ³å°¿å°¿")))
    
    # æ¸¬è©¦ 2: ä¸€èˆ¬ LLM Router
    print("\n--- Test 2: LLM Router (No History) ---")
    print(asyncio.run(p.process("ä½ å¥½æˆ‘æƒ³çŸ¥é“ä½ å€‘æœ‰è³£å“ªäº›æ¡ŒéŠ")))
    
    # æ¸¬è©¦ 3: Context Aware (æ¨¡æ“¬ Client å¸¶å…¥ History)
    print("\n--- Test 3: Context Injection (Simulate Client History) ---")
    # æƒ…å¢ƒï¼šä¸Šä¸€è¼ªå•äº†åƒ¹æ ¼ï¼Œé€™ä¸€è¼ªåªå•ã€Œé‚£å‡æ—¥å‘¢ï¼Ÿã€
    mock_history = [
        {"role": "user", "content": "å¹³æ—¥å¤šå°‘éŒ¢ï¼Ÿ", "intent": "STORE_PRICING"},
        {"role": "assistant", "content": "å¹³æ—¥ 60 å…ƒ..."}
    ]
    # æˆ‘å€‘æœŸæœ›é€™å¥æ¨¡ç³Šçš„ã€Œé‚£å‡æ—¥å‘¢ã€èƒ½å› ç‚º History è€Œè¢«è­˜åˆ¥æ­£ç¢º
    print(f"Input: é‚£å‡æ—¥å‘¢ï¼Ÿ (with context: STORE_PRICING)")
    print(asyncio.run(p.process("é‚£å‡æ—¥å‘¢ï¼Ÿ", history=mock_history)))
    
    print("\n=============================")
    print("Tests Completed.")