"""
Project Akka - Pipeline Module
Orchestrator for Hybrid Routing (Semantic -> LLM)
Refactored for Stateless Architecture (v9.6)
"""

import logging
import asyncio
import random
from pathlib import Path
from typing import Optional, List, Dict, Any
from dataclasses import dataclass

try:
    from .llm import LLMServiceManager
except ImportError:
    from llm.manager import LLMServiceManager

# Project modules
try:
    from .boardgame_utils import ConfigLoader, PromptManager
    from .data_manager import get_data_manager
    from .semantic_router import SemanticRouter
except ImportError:
    from boardgame_utils import ConfigLoader, PromptManager
    from data_manager import get_data_manager
    from semantic_router import SemanticRouter

logger = logging.getLogger(__name__)

@dataclass
class RouterResult:
    intent: str
    confidence: float = 0.0
    source: str = "unknown"

@dataclass
class PipelineResult:
    response: str
    intent: Optional[str] = None
    confidence: float = 0.0
    source: str = "unknown"

class Pipeline:
    def __init__(self, config_dir: Optional[Path] = None):
        self.config_dir = config_dir or Path(__file__).parent.parent / "config"
        self.data_manager = get_data_manager()
        self.system_config = {}
        self.semantic_routes = {}
        
        # 1. Load Configurations
        self._load_configs()
        
        # 2. Initialize Semantic Router
        embedding_config = self.system_config.get("model", {}).get("embedding", {})
        self.semantic_router = SemanticRouter(
            model_config=embedding_config,
            routes_config=self.semantic_routes
        )
        
        # 3. Initialize LLM Manager
        self.llm_manager = LLMServiceManager(self.system_config)
        self.local_llm = self.llm_manager.get_local()
        self.cloud_llm = self.llm_manager.get_cloud()

    def _load_configs(self) -> None:
        """Load all YAML configurations."""
        try:
            self.store_info = ConfigLoader(self.config_dir / "store_info.yaml").load()
            self.intent_map = ConfigLoader(self.config_dir / "intent_map.yaml").load()
            self.local_prompts = PromptManager(self.config_dir / "prompts_local.yaml")
            self.cloud_prompts = PromptManager(self.config_dir / "prompts_cloud.yaml")
            self.system_config = ConfigLoader(self.config_dir / "system_config.yaml").load()
            
            # Optional Configs
            try:
                self.semantic_routes = ConfigLoader(self.config_dir / "semantic_routes.yaml").load()
            except Exception:
                logger.warning("semantic_routes.yaml missing.")
                self.semantic_routes = {}
            
            logger.info("Config loaded.")
        except Exception as e:
            logger.error(f"Config load failed: {e}")
            self.store_info = {}
            self.semantic_routes = {}
            self.intent_map = {}
            self.system_config = {}

    def reload_configs(self) -> None:
        logger.info("Reloading configurations...")
        self._load_configs()
        embedding_config = self.system_config.get("model", {}).get("embedding", {})
        self.semantic_router = SemanticRouter(embedding_config, self.semantic_routes)

    async def process(
        self, 
        user_input: str, 
        history: List[Dict[str, Any]] = None, 
        game_context: Dict[str, Any] = None, # <--- æ–°å¢é€™è£¡
        llm_service=None
    ) -> PipelineResult:
        """
        Main processing pipeline.
        Args:
            user_input: The current user query.
            history: List of past turns (provided by Client) to extract context.
        """
        user_input = user_input.strip()
        if not user_input:
            return PipelineResult(response="...", source="empty")

        # ============================================================
        # [NEW] Stage 0: Context Extraction (Stateless Logic)
        # ============================================================
        context_str = ""
        if history:
            # ç¯©é¸è¦å‰‡ï¼šåªçœ‹ User çš„ç™¼è¨€ï¼Œä¸”è©²ç™¼è¨€å¿…é ˆå¸¶æœ‰ intent
            recent_user_logs = [
                msg for msg in history 
                if msg.get("role") == "user" and msg.get("intent")
            ]
            
            if recent_user_logs:
                # å–å‡ºæœ€å¾Œ 2 æ¬¡çš„æ„åœ–è»Œè·¡ (ä¾‹å¦‚: RULES -> STORE_PRICING)
                last_intents = [msg["intent"] for msg in recent_user_logs[-2:]]
                context_str = " -> ".join(last_intents)
                logger.info(f"ğŸ•µï¸ Context Extracted from Request: {context_str}")

        # --- Stage 1: Semantic Vector Routing (FastPath) ---
        semantic_intent, score = self.semantic_router.route(user_input)
    
        if semantic_intent:
            logger.info(f"âš¡ FastPath Hit: {semantic_intent} (Score: {score:.4f})")
            response, source = await self._dispatch(semantic_intent, user_input, None)
            return PipelineResult(
                response=response,
                intent=semantic_intent,
                confidence=float(score),
                source=f"fastpath_{source}"
            )

        # --- Stage 2: LLM Intent Routing ---
        if self.llm_manager is None:
            return PipelineResult(response="ç³»çµ±ç¶­è­·ä¸­...", source="error")

        logger.info("ğŸ¢ FastPath Miss. Engaging LLM Router...")
    
        # [MODIFY] å°‡ Context æ³¨å…¥ Prompt
        if context_str:
            final_input = f"[Context: {context_str}] User Input: {user_input}"
        else:
            final_input = user_input

        # å‚³é€ final_input çµ¦ Router
        router_result = await self._route_with_llm(final_input, self.local_llm)
        # ä½¿ç”¨ Logger è€Œä¸æ˜¯ Printï¼Œä¿æŒ Log ä¹¾æ·¨
        logger.info(f"Router Result: {router_result}") 

        # --- Stage 3: Safety Filter ---
        if router_result.intent == "SENSITIVE":
            if self._check_allowlist(user_input):
                router_result.intent = "RULES"
        context_pack = {
            "history": history,
            "game_context": game_context
        }
        # --- Stage 4: Dispatch ---
        response, source = await self._dispatch(
            router_result.intent, 
            user_input, 
            context_pack
        )
        
        return PipelineResult(
            response=response,
            intent=router_result.intent,
            confidence=router_result.confidence,
            source=source
        )

    async def _route_with_llm(self, final_input: str, llm_service) -> RouterResult:
        """
        Sends the constructed input (with context) to the Local LLM.
        """
        router_config = self.local_prompts.get_task_config("router")
        system_prompt = router_config.get("system_prompt", "")
        
        if not system_prompt:
            logger.warning("Router system prompt is empty!")
            
        if not self.local_llm:
            return RouterResult(intent="UNKNOWN", confidence=0.0, source="error")
            
        try:
            # ç›´æ¥ä½¿ç”¨å·²ç¶“çµ„å¥½çš„ final_input
            response = await self.local_llm.generate_json(final_input, system_prompt)
            intent = response.get("intent", "UNKNOWN")
            confidence = response.get("confidence", 0.0)
            return RouterResult(intent=intent, confidence=confidence, source="llm")
        except Exception as e:
            logger.error(f"Router LLM Error: {e}")
            return RouterResult(intent="UNKNOWN", confidence=0.0, source="fallback")

    def _check_allowlist(self, user_input: str) -> bool:
        try:
            games = self.data_manager.list_games()
            for game in games:
                allowlist = game.metadata.get("allowlist_keywords", [])
                for keyword in allowlist:
                    if keyword in user_input: return True
        except Exception:
            pass
        return False

    async def _dispatch(self, intent: str, user_input: str, context: Any = None) -> tuple[str, str]:
        responses_map = self.store_info.get("responses", {})
        
        # 1. Static Responses
        if intent in responses_map:
            candidates = responses_map[intent]
            if candidates: return (random.choice(candidates), "content_static")
        
        # 2. Logic Handlers
        logic_intents = self.intent_map.get("logic_intents", {})
        if intent in logic_intents:
            logic_config = logic_intents[intent]
            handler = logic_config.get("handler", "")
            
            if handler == "local_llm" and self.local_llm:
                # é€™è£¡èª¿ç”¨ Persona é€²è¡Œé–’èŠ
                task = logic_config.get("task", "casual_chat")
                sys_prompt = self.local_prompts.get_task_config("casual_chat").get("system_prompt", "")
                resp = await self.local_llm.generate(user_input, sys_prompt)
                return (resp.content, "local_llm_gen")
            elif handler in ["online_llm", "cloud_llm", "cloud_rag"]:
                if self.cloud_llm:
                    return await self._handle_rules_query(user_input, context)
                else:
                    return ("æŠ±æ­‰ï¼Œé›²ç«¯å¤§è…¦é€£ç·šæœ‰é»å•é¡Œï¼Œè«‹ç¨å¾Œå†è©¦ã€‚", "error")
            elif handler == "reject":
                return (logic_config.get("response", "æŠ±æ­‰"), "reject")

        # 3. Fallback
        fallback = self.store_info.get("responses", {}).get("UNKNOWN_FALLBACK", ["æŠ±æ­‰ï¼Ÿ"])
        return (random.choice(fallback), "fallback")
    # src/pipeline.py -> class Pipeline

    # [æ–°å¢] æ•´å€‹å‡½å¼
    async def _handle_rules_query(self, user_input: str, context: Dict[str, Any]) -> tuple[str, str]:
        """
        å°ˆé–€è™•ç† RULES æ„åœ–çš„é‚è¼¯å‡½å¼
        ä¿®æ­£é‡é»ï¼šå°‡ history æ ¼å¼åŒ–ä¸¦æ³¨å…¥ System Prompt
        """
        # 1. å–å¾—éŠæˆ²åç¨± & æ­·å²ç´€éŒ„
        ctx = context if isinstance(context, dict) else {}
        game_ctx = ctx.get("game_context", {}) or {}
        game_id = game_ctx.get("game_id", "carcassonne") 
        history = ctx.get("history", []) # å–å¾—æ­·å²ç´€éŒ„ List
        
        # 2. é€é DataManager å–å¾—è¦å‰‡å…§å®¹
        rule_content = self.data_manager.get_rules(game_id)
        if not rule_content:
            logger.warning(f"Rulebook not found for game_id: {game_id}")
            rule_content = "ï¼ˆç³»çµ±æç¤ºï¼šç›®å‰æ‰¾ä¸åˆ°æ­¤éŠæˆ²çš„è©³ç´°è¦å‰‡è³‡æ–™ï¼Œè«‹ä¾æ“šæ‚¨çš„é€šç”¨çŸ¥è­˜å›ç­”ã€‚ï¼‰"

        # 3. è®€å– System Prompt Template
        # å‡è¨­ prompts_cloud.yaml è£¡æœ‰ {INJECTED_RAG_CONTENT} å’Œ {history} å…©å€‹ä½”ä½ç¬¦
        task_config = self.cloud_prompts.get_task_config("rules_explainer")
        system_template = task_config.get("system_prompt", "")
        
        # 4. [é—œéµä¿®æ­£] æ ¼å¼åŒ– History
        history_str = ""
        if history:
            history_lines = []
            for msg in history:
                role = msg.get("role", "unknown")
                content = msg.get("content", "")
                # éæ¿¾æ‰å¤ªé•·çš„æ­·å²ç´€éŒ„ä»¥ç¯€çœ Tokenï¼Œæˆ–åªä¿ç•™ RULES ç›¸é—œçš„
                # é€™è£¡ç°¡å–®å…¨ç•™ï¼Œæ¨™è¨»è§’è‰²å³å¯
                history_lines.append(f"{role}: {content}")
            history_str = "\n".join(history_lines)
        else:
            history_str = "(No previous conversation)"

        # 5. æ³¨å…¥è®Šæ•¸ (è¦å‰‡ + æ­·å²)
        # æ³¨æ„ï¼šé€™è£¡ä½¿ç”¨ replace ç°¡å–®æ›¿æ›ã€‚å»ºè­°ç¢ºèª YAML è£¡çš„ä½”ä½ç¬¦åç¨±æ˜¯å¦ä¸€è‡´ã€‚
        final_system_prompt = (
            system_template
            .replace("{INJECTED_RAG_CONTENT}", rule_content)
            .replace("{history}", history_str) 
        )
        
        # 6. å‘¼å«é›²ç«¯å¤§è…¦
        try:
            # å‘¼å« Cloud LLM
            # user_input æ˜¯ç•¶å‰ä½¿ç”¨è€…çš„å•é¡Œ
            raw_response = await self.cloud_llm.generate(
                user_input,
                system_prompt=final_system_prompt
            )
            
            response_text = raw_response.content if hasattr(raw_response, "content") else str(raw_response)
            return (response_text, "cloud_gen")
            
        except Exception as e:
            logger.error(f"Cloud Handling Error: {e}")
            return ("æŠ±æ­‰ï¼Œé›²ç«¯å¤§è…¦é€£ç·šæœ‰é»å•é¡Œï¼Œè«‹ç¨å¾Œå†è©¦ã€‚", "error")

def create_pipeline(config_dir: Optional[Path] = None) -> Pipeline:
    return Pipeline(config_dir=config_dir)

if __name__ == "__main__":
    # Test Standalone Pipeline
    import time 
    logging.basicConfig(level=logging.INFO)
    p = create_pipeline()
    print("Pipeline Initialized.")
    print("=============================")
    
    # æ¸¬è©¦ 1: ä¸€èˆ¬ FastPath
    print("\n--- Test 1: FastPath (No History) ---")
    print(asyncio.run(p.process("æƒ³å°¿å°¿")))
    
    # æ¸¬è©¦ 2: ä¸€èˆ¬ LLM Router
    print("\n--- Test 2: LLM Router (No History) ---")
    print(asyncio.run(p.process("ä½ å¥½æˆ‘æƒ³çŸ¥é“ä½ å€‘æœ‰è³£å“ªäº›æ¡ŒéŠ")))
    
    # æ¸¬è©¦ 3: Context Aware (æ¨¡æ“¬ Client å¸¶å…¥ History)
    print("\n--- Test 3: Context Injection (Simulate Client History) ---")
    # æƒ…å¢ƒï¼šä¸Šä¸€è¼ªå•äº†åƒ¹æ ¼ï¼Œé€™ä¸€è¼ªåªå•ã€Œé‚£å‡æ—¥å‘¢ï¼Ÿã€
    mock_history = [
        {"role": "user", "content": "å¹³æ—¥å¤šå°‘éŒ¢ï¼Ÿ", "intent": "STORE_PRICING"},
        {"role": "assistant", "content": "å¹³æ—¥ 60 å…ƒ..."}
    ]
    # æˆ‘å€‘æœŸæœ›é€™å¥æ¨¡ç³Šçš„ã€Œé‚£å‡æ—¥å‘¢ã€èƒ½å› ç‚º History è€Œè¢«è­˜åˆ¥æ­£ç¢º
    print(f"Input: é‚£å‡æ—¥å‘¢ï¼Ÿ (with context: STORE_PRICING)")
    print(asyncio.run(p.process("é‚£å‡æ—¥å‘¢ï¼Ÿ", history=mock_history)))
    
    print("\n=============================")
    print("Tests Completed.")