"""
Project Akka - Pipeline Module
Orchestrator for Hybrid Routing (Semantic -> LLM)
Refactored for Stateless Architecture (v9.6)
"""

import logging
import asyncio
import random
from pathlib import Path
from typing import Optional, List, Dict, Any
from dataclasses import dataclass

try:
    from .llm import LLMServiceManager
except ImportError:
    from llm.manager import LLMServiceManager

# Project modules
try:
    from .boardgame_utils import ConfigLoader, PromptManager
    from .data_manager import get_data_manager
    from .semantic_router import SemanticRouter
except ImportError:
    from boardgame_utils import ConfigLoader, PromptManager
    from data_manager import get_data_manager
    from semantic_router import SemanticRouter

logger = logging.getLogger(__name__)

@dataclass
class RouterResult:
    intent: str
    confidence: float = 0.0
    source: str = "unknown"

@dataclass
class PipelineResult:
    response: str
    intent: Optional[str] = None
    confidence: float = 0.0
    source: str = "unknown"

class Pipeline:
    def __init__(self, config_dir: Optional[Path] = None):
        self.config_dir = config_dir or Path(__file__).parent.parent / "config"
        self.data_manager = get_data_manager()
        self.system_config = {}
        self.semantic_routes = {}
        
        # 1. Load Configurations
        self._load_configs()
        
        # 2. Initialize Semantic Router
        embedding_config = self.system_config.get("model", {}).get("embedding", {})
        self.semantic_router = SemanticRouter(
            model_config=embedding_config,
            routes_config=self.semantic_routes
        )
        
        # 3. Initialize LLM Manager
        self.llm_manager = LLMServiceManager(self.system_config)
        self.local_llm = self.llm_manager.get_local()
        self.cloud_llm = self.llm_manager.get_cloud()

    def _load_configs(self) -> None:
        """Load all YAML configurations."""
        try:
            self.store_info = ConfigLoader(self.config_dir / "store_info.yaml").load()
            self.intent_map = ConfigLoader(self.config_dir / "intent_map.yaml").load()
            self.local_prompts = PromptManager(self.config_dir / "prompts_local.yaml")
            self.cloud_prompts = PromptManager(self.config_dir / "prompts_cloud.yaml")
            self.system_config = ConfigLoader(self.config_dir / "system_config.yaml").load()
            
            # Optional Configs
            try:
                self.semantic_routes = ConfigLoader(self.config_dir / "semantic_routes.yaml").load()
            except Exception:
                logger.warning("semantic_routes.yaml missing.")
                self.semantic_routes = {}
            
            logger.info("Config loaded.")
        except Exception as e:
            logger.error(f"Config load failed: {e}")
            self.store_info = {}
            self.semantic_routes = {}
            self.intent_map = {}
            self.system_config = {}

    def reload_configs(self) -> None:
        logger.info("Reloading configurations...")
        self._load_configs()
        embedding_config = self.system_config.get("model", {}).get("embedding", {})
        self.semantic_router = SemanticRouter(embedding_config, self.semantic_routes)

    async def process(self, user_input: str, history: List[Dict[str, Any]] = None, llm_service=None) -> PipelineResult:
        """
        Main processing pipeline.
        Args:
            user_input: The current user query.
            history: List of past turns (provided by Client) to extract context.
        """
        user_input = user_input.strip()
        if not user_input:
            return PipelineResult(response="...", source="empty")

        # ============================================================
        # [NEW] Stage 0: Context Extraction (Stateless Logic)
        # ============================================================
        context_str = ""
        if history:
            # ç¯©é¸è¦å‰‡ï¼šåªçœ‹ User çš„ç™¼è¨€ï¼Œä¸”è©²ç™¼è¨€å¿…é ˆå¸¶æœ‰ intent
            recent_user_logs = [
                msg for msg in history 
                if msg.get("role") == "user" and msg.get("intent")
            ]
            
            if recent_user_logs:
                # å–å‡ºæœ€å¾Œ 2 æ¬¡çš„æ„åœ–è»Œè·¡ (ä¾‹å¦‚: RULES -> STORE_PRICING)
                last_intents = [msg["intent"] for msg in recent_user_logs[-2:]]
                context_str = " -> ".join(last_intents)
                logger.info(f"ğŸ•µï¸ Context Extracted from Request: {context_str}")

        # --- Stage 1: Semantic Vector Routing (FastPath) ---
        semantic_intent, score = self.semantic_router.route(user_input)
    
        if semantic_intent:
            logger.info(f"âš¡ FastPath Hit: {semantic_intent} (Score: {score:.4f})")
            response, source = await self._dispatch(semantic_intent, user_input, None)
            return PipelineResult(
                response=response,
                intent=semantic_intent,
                confidence=float(score),
                source=f"fastpath_{source}"
            )

        # --- Stage 2: LLM Intent Routing ---
        if self.llm_manager is None:
            return PipelineResult(response="ç³»çµ±ç¶­è­·ä¸­...", source="error")

        logger.info("ğŸ¢ FastPath Miss. Engaging LLM Router...")
    
        # [MODIFY] å°‡ Context æ³¨å…¥ Prompt
        if context_str:
            final_input = f"[Context: {context_str}] User Input: {user_input}"
        else:
            final_input = user_input

        # å‚³é€ final_input çµ¦ Router
        router_result = await self._route_with_llm(final_input, self.local_llm)
        # ä½¿ç”¨ Logger è€Œä¸æ˜¯ Printï¼Œä¿æŒ Log ä¹¾æ·¨
        logger.info(f"Router Result: {router_result}") 

        # --- Stage 3: Safety Filter ---
        if router_result.intent == "SENSITIVE":
            if self._check_allowlist(user_input):
                router_result.intent = "RULES"

        # --- Stage 4: Dispatch ---
        response, source = await self._dispatch(
            router_result.intent, 
            user_input, 
            llm_service
        )
        
        return PipelineResult(
            response=response,
            intent=router_result.intent,
            confidence=router_result.confidence,
            source=source
        )

    async def _route_with_llm(self, final_input: str, llm_service) -> RouterResult:
        """
        Sends the constructed input (with context) to the Local LLM.
        """
        router_config = self.local_prompts.get_task_config("router")
        system_prompt = router_config.get("system_prompt", "")
        
        if not system_prompt:
            logger.warning("Router system prompt is empty!")
            
        if not self.local_llm:
            return RouterResult(intent="UNKNOWN", confidence=0.0, source="error")
            
        try:
            # ç›´æ¥ä½¿ç”¨å·²ç¶“çµ„å¥½çš„ final_input
            response = await self.local_llm.generate_json(final_input, system_prompt)
            intent = response.get("intent", "UNKNOWN")
            confidence = response.get("confidence", 0.0)
            return RouterResult(intent=intent, confidence=confidence, source="llm")
        except Exception as e:
            logger.error(f"Router LLM Error: {e}")
            return RouterResult(intent="UNKNOWN", confidence=0.0, source="fallback")

    def _check_allowlist(self, user_input: str) -> bool:
        try:
            games = self.data_manager.list_games()
            for game in games:
                allowlist = game.metadata.get("allowlist_keywords", [])
                for keyword in allowlist:
                    if keyword in user_input: return True
        except Exception:
            pass
        return False

    async def _dispatch(self, intent: str, user_input: str, llm_service) -> tuple[str, str]:
        responses_map = self.store_info.get("responses", {})
        
        # 1. Static Responses
        if intent in responses_map:
            candidates = responses_map[intent]
            if candidates: return (random.choice(candidates), "content_static")
        
        # 2. Logic Handlers
        logic_intents = self.intent_map.get("logic_intents", {})
        if intent in logic_intents:
            logic_config = logic_intents[intent]
            handler = logic_config.get("handler", "")
            
            if handler == "local_llm" and self.local_llm:
                # é€™è£¡èª¿ç”¨ Persona é€²è¡Œé–’èŠ
                task = logic_config.get("task", "casual_chat")
                sys_prompt = self.local_prompts.get_task_config("casual_chat").get("system_prompt", "")
                resp = await self.local_llm.generate(user_input, sys_prompt)
                return (resp.content, "local_llm_gen")
                
            elif handler == "reject":
                return (logic_config.get("response", "æŠ±æ­‰"), "reject")

        # 3. Fallback
        fallback = self.store_info.get("responses", {}).get("UNKNOWN_FALLBACK", ["æŠ±æ­‰ï¼Ÿ"])
        return (random.choice(fallback), "fallback")

def create_pipeline(config_dir: Optional[Path] = None) -> Pipeline:
    return Pipeline(config_dir=config_dir)

if __name__ == "__main__":
    # Test Standalone Pipeline
    import time 
    logging.basicConfig(level=logging.INFO)
    p = create_pipeline()
    print("Pipeline Initialized.")
    print("=============================")
    
    # æ¸¬è©¦ 1: ä¸€èˆ¬ FastPath
    print("\n--- Test 1: FastPath (No History) ---")
    print(asyncio.run(p.process("æƒ³å°¿å°¿")))
    
    # æ¸¬è©¦ 2: ä¸€èˆ¬ LLM Router
    print("\n--- Test 2: LLM Router (No History) ---")
    print(asyncio.run(p.process("ä½ å¥½æˆ‘æƒ³çŸ¥é“ä½ å€‘æœ‰è³£å“ªäº›æ¡ŒéŠ")))
    
    # æ¸¬è©¦ 3: Context Aware (æ¨¡æ“¬ Client å¸¶å…¥ History)
    print("\n--- Test 3: Context Injection (Simulate Client History) ---")
    # æƒ…å¢ƒï¼šä¸Šä¸€è¼ªå•äº†åƒ¹æ ¼ï¼Œé€™ä¸€è¼ªåªå•ã€Œé‚£å‡æ—¥å‘¢ï¼Ÿã€
    mock_history = [
        {"role": "user", "content": "å¹³æ—¥å¤šå°‘éŒ¢ï¼Ÿ", "intent": "STORE_PRICING"},
        {"role": "assistant", "content": "å¹³æ—¥ 60 å…ƒ..."}
    ]
    # æˆ‘å€‘æœŸæœ›é€™å¥æ¨¡ç³Šçš„ã€Œé‚£å‡æ—¥å‘¢ã€èƒ½å› ç‚º History è€Œè¢«è­˜åˆ¥æ­£ç¢º
    print(f"Input: é‚£å‡æ—¥å‘¢ï¼Ÿ (with context: STORE_PRICING)")
    print(asyncio.run(p.process("é‚£å‡æ—¥å‘¢ï¼Ÿ", history=mock_history)))
    
    print("\n=============================")
    print("Tests Completed.")