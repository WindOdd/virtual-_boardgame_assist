# System Configuration for Project Akka
# Targeting NVIDIA Jetson Orin Nano

server:
  host: "0.0.0.0"
  port: 8000
  concurrency: 8

model:
  local:
    name: "qwen3:4b-instruct"
    provider: "ollama"
    description: "Local LLM running on Jetson Orin Nano"
  cloud:
    name: "gemini-2.5-flash"
    provider: "google"
    description: "Cloud LLM for complex reasoning tasks"

# Hardware optimization hints for Jetson Orin Nano
hardware:
  target_device: "jetson_orin_nano"
  gpu_memory_fraction: 0.8
  enable_tensor_cores: true
