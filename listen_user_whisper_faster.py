"""
æ¡ŒéŠèªéŸ³åŠ©ç† - STT æ ¸å¿ƒç³»çµ± (faster-whisper ç‰ˆæœ¬)
åŠŸèƒ½ï¼šéŸ³è¨Šè¼¸å…¥ â†’ VAD â†’ STT (å®Œå…¨è¨˜æ†¶é«”è™•ç†)

å„ªå‹¢ï¼š
- æ›´å¿«çš„è½‰éŒ„é€Ÿåº¦ï¼ˆ2-3å€ï¼‰
- ç„¡ Windows æª”æ¡ˆé–å®šå•é¡Œ
- ç›´æ¥è™•ç† numpy array
- æ¨™æº–åŒ–çš„è¿”å›æ ¼å¼
"""

import sounddevice as sd
import numpy as np
import time
from datetime import datetime
from collections import deque
from pathlib import Path
import threading

# æª¢æŸ¥ä¾è³´
try:
    from faster_whisper import WhisperModel
except ImportError:
    print("âŒ è«‹å®‰è£: pip install faster-whisper")
    exit(1)

try:
    import torch
except ImportError:
    print("âŒ è«‹å®‰è£: pip install torch")
    exit(1)

try:
    from scipy.io import wavfile
except ImportError:
    print("âŒ è«‹å®‰è£: pip install scipy")
    exit(1)


# ==================== é…ç½®é¡ ====================
class Config:
    """ç³»çµ±é…ç½®"""
    # éŸ³è¨Šåƒæ•¸
    SAMPLE_RATE = 16000          # æ¡æ¨£ç‡ (Hz)
    CHANNELS = 1                 # å–®è²é“
    DTYPE = 'int16'              # æ•¸æ“šé¡å‹
    
    # éŒ„éŸ³æ§åˆ¶
    MAX_RECORDING_DURATION = 30  # æœ€å¤§éŒ„éŸ³æ™‚é•·ï¼ˆç§’ï¼‰
    SILENCE_DURATION = 1.5       # éœéŸ³åˆ¤å®šæ™‚é•·ï¼ˆç§’ï¼‰
    SILENCE_THRESHOLD = 50       # éœéŸ³èƒ½é‡é–¾å€¼
    
    # VAD åƒæ•¸
    VAD_THRESHOLD = 0.5          # Silero VAD é–¾å€¼ (0-1)
    MIN_SPEECH_DURATION = 0.5    # æœ€çŸ­æœ‰æ•ˆèªéŸ³æ™‚é•·ï¼ˆç§’ï¼‰
    MIN_SPEECH_ENERGY = 100      # æœ€å°èªéŸ³èƒ½é‡ï¼ˆå‚™ç”¨æ–¹æ¡ˆï¼‰
    
    # Whisper è¨­å®š
    WHISPER_MODEL = "base"       # æ¨¡å‹å¤§å°: tiny, base, small, medium, large
    WHISPER_DEVICE = "cpu"       # è¨­å‚™: cpu, cuda
    WHISPER_COMPUTE_TYPE = "int8" # è¨ˆç®—é¡å‹: int8, float16, float32
    WHISPER_LANGUAGE = "zh"      # èªè¨€
    
    # ç³»çµ±è¨­å®š
    DEBUG_MODE = True            # é™¤éŒ¯æ¨¡å¼ï¼ˆæœƒä¿å­˜éŸ³è¨Šæª”æ¡ˆï¼‰
    LOG_DIR = Path("logs")       # æ—¥èªŒç›®éŒ„
    
    @classmethod
    def print_config(cls):
        """åˆ—å°ç•¶å‰é…ç½®"""
        print("\n" + "="*60)
        print("ç³»çµ±é…ç½®")
        print("="*60)
        print(f"æ¡æ¨£ç‡: {cls.SAMPLE_RATE} Hz")
        print(f"Whisper æ¨¡å‹: {cls.WHISPER_MODEL}")
        print(f"è¨ˆç®—é¡å‹: {cls.WHISPER_COMPUTE_TYPE}")
        print(f"è¨­å‚™: {cls.WHISPER_DEVICE}")
        print(f"èªè¨€: {cls.WHISPER_LANGUAGE}")
        print(f"é™¤éŒ¯æ¨¡å¼: {'é–‹å•Ÿ' if cls.DEBUG_MODE else 'é—œé–‰'}")
        print("="*60 + "\n")


# ==================== éŸ³è¨Šç·©è¡å€ ====================
class AudioBuffer:
    """å¾ªç’°éŸ³è¨Šç·©è¡å€ï¼ˆè¨˜æ†¶é«”ç®¡ç†ï¼‰"""
    
    def __init__(self, max_duration, sample_rate):
        self.max_samples = int(max_duration * sample_rate)
        self.buffer = deque(maxlen=self.max_samples)
        self.sample_rate = sample_rate
        self.lock = threading.Lock()
    
    def add(self, data):
        """æ·»åŠ éŸ³è¨Šæ•¸æ“šï¼ˆç·šç¨‹å®‰å…¨ï¼‰"""
        with self.lock:
            self.buffer.extend(data.flatten())
    
    def clear(self):
        """æ¸…ç©ºç·©è¡å€"""
        with self.lock:
            self.buffer.clear()
    
    def get_array(self):
        """ç²å–å®Œæ•´éŸ³è¨Šæ•¸æ“šï¼ˆfloat32 æ ¼å¼ï¼‰"""
        with self.lock:
            # faster-whisper éœ€è¦ float32 [-1, 1]
            audio_int16 = np.array(list(self.buffer), dtype='int16')
            audio_float32 = audio_int16.astype(np.float32) / 32768.0
            return audio_float32
    
    def get_duration(self):
        """ç²å–ç•¶å‰ç·©è¡æ™‚é•·"""
        with self.lock:
            return len(self.buffer) / self.sample_rate


# ==================== VAD è™•ç†å™¨ ====================
class VADProcessor:
    """èªéŸ³æ´»å‹•æª¢æ¸¬å™¨"""
    
    def __init__(self, sample_rate=16000, threshold=0.5):
        self.sample_rate = sample_rate
        self.threshold = threshold
        self.model = None
        self.utils = None
        self._load_model()
    
    def _load_model(self):
        """è¼‰å…¥ Silero VAD æ¨¡å‹"""
        try:
            print("â³ è¼‰å…¥ Silero VAD æ¨¡å‹...")
            self.model, self.utils = torch.hub.load(
                repo_or_dir='snakers4/silero-vad',
                model='silero_vad',
                force_reload=False,
                onnx=False,
                verbose=False
            )
            self.model.eval()
            print("âœ… Silero VAD å·²è¼‰å…¥")
        except Exception as e:
            print(f"âš ï¸  Silero VAD è¼‰å…¥å¤±æ•—: {e}")
            print("   å°‡ä½¿ç”¨ç°¡å–®èƒ½é‡æª¢æ¸¬ä½œç‚ºå‚™ç”¨æ–¹æ¡ˆ")
            self.model = None
    
    def has_speech(self, audio_float32):
        """
        æª¢æ¸¬éŸ³è¨Šä¸­æ˜¯å¦åŒ…å«èªéŸ³
        
        Args:
            audio_float32: numpy array (float32, [-1, 1])
        """
        if self.model is None:
            return self._energy_based_vad(audio_float32)
        
        try:
            return self._silero_vad(audio_float32)
        except Exception as e:
            print(f"âš ï¸  VAD æª¢æ¸¬éŒ¯èª¤: {e}ï¼Œä½¿ç”¨å‚™ç”¨æ–¹æ¡ˆ")
            return self._energy_based_vad(audio_float32)
    
    def _silero_vad(self, audio_float32):
        """ä½¿ç”¨ Silero VAD æª¢æ¸¬"""
        audio_tensor = torch.from_numpy(audio_float32)
        
        get_speech_timestamps = self.utils[0]
        speech_timestamps = get_speech_timestamps(
            audio_tensor,
            self.model,
            sampling_rate=self.sample_rate,
            threshold=self.threshold,
            min_speech_duration_ms=int(Config.MIN_SPEECH_DURATION * 1000),
            return_seconds=False
        )
        
        if not speech_timestamps:
            return False
        
        total_speech_samples = sum(
            ts['end'] - ts['start'] 
            for ts in speech_timestamps
        )
        total_speech_duration = total_speech_samples / self.sample_rate
        
        return total_speech_duration >= Config.MIN_SPEECH_DURATION
    
    def _energy_based_vad(self, audio_float32):
        """ç°¡å–®èƒ½é‡æª¢æ¸¬ï¼ˆå‚™ç”¨æ–¹æ¡ˆï¼‰"""
        energy = np.mean(np.abs(audio_float32))
        # float32 çš„èƒ½é‡é–¾å€¼éœ€è¦èª¿æ•´
        return energy > (Config.MIN_SPEECH_ENERGY / 32768.0)


# ==================== faster-whisper STT ====================
class FasterWhisperSTT:
    """faster-whisper èªéŸ³è½‰æ–‡å­—ï¼ˆå®Œå…¨è¨˜æ†¶é«”è™•ç†ï¼‰"""
    
    def __init__(self, model_size="base", device="cpu", compute_type="int8", language="zh"):
        """
        åˆå§‹åŒ– faster-whisper
        
        Args:
            model_size: æ¨¡å‹å¤§å° (tiny, base, small, medium, large)
            device: é‹è¡Œè¨­å‚™ (cpu, cuda)
            compute_type: è¨ˆç®—é¡å‹ (int8, float16, float32)
            language: èªè¨€ä»£ç¢¼
        """
        self.language = language
        self.model = None
        self._load_model(model_size, device, compute_type)
    
    def _load_model(self, model_size, device, compute_type):
        """è¼‰å…¥ faster-whisper æ¨¡å‹"""
        try:
            print(f"â³ è¼‰å…¥ faster-whisper æ¨¡å‹: {model_size}")
            print(f"   è¨­å‚™: {device}, è¨ˆç®—é¡å‹: {compute_type}")
            
            self.model = WhisperModel(
                model_size,
                device=device,
                compute_type=compute_type
            )
            
            print("âœ… faster-whisper å·²è¼‰å…¥")
            
        except Exception as e:
            print(f"âŒ faster-whisper è¼‰å…¥å¤±æ•—: {e}")
            import traceback
            traceback.print_exc()
            self.model = None
    
    def transcribe(self, audio_float32):
        """
        è½‰éŒ„éŸ³è¨Šï¼ˆå®Œå…¨è¨˜æ†¶é«”è™•ç†ï¼‰
        
        Args:
            audio_float32: numpy array (float32, [-1, 1])
            
        Returns:
            str: è½‰éŒ„æ–‡å­—
        """
        if self.model is None:
            print("âŒ faster-whisper æ¨¡å‹æœªè¼‰å…¥")
            return ""
        
        try:
            start_time = time.time()
            
            # faster-whisper å¯ä»¥ç›´æ¥æ¥å— numpy arrayï¼
            segments, info = self.model.transcribe(
                audio_float32,
                language=self.language,
                beam_size=5,
                vad_filter=False,  # æˆ‘å€‘å·²ç¶“ç”¨ Silero VAD äº†
                without_timestamps=True  # ä¸éœ€è¦æ™‚é–“æˆ³ï¼Œæ›´å¿«
            )
            
            # çµ„åˆæ‰€æœ‰ segment çš„æ–‡å­—
            text = "".join([segment.text for segment in segments]).strip()
            
            elapsed = time.time() - start_time
            
            # é¡¯ç¤ºè¾¨è­˜è³‡è¨Š
            print(f"â±ï¸  è½‰éŒ„è€—æ™‚: {elapsed:.2f} ç§’")
            print(f"ğŸ“Š æª¢æ¸¬èªè¨€: {info.language} (ç½®ä¿¡åº¦: {info.language_probability:.2%})")
            
            return text
            
        except Exception as e:
            print(f"âŒ è½‰éŒ„éŒ¯èª¤: {e}")
            import traceback
            traceback.print_exc()
            return ""


# ==================== éŒ„éŸ³ç®¡ç†å™¨ ====================
class AudioRecorder:
    """éŸ³è¨ŠéŒ„éŸ³ç®¡ç†å™¨"""
    
    def __init__(self, buffer):
        self.buffer = buffer
        self.stream = None
        self.is_recording = False
        self.silence_start = None
    
    def _callback(self, indata, frames, time_info, status):
        """éŸ³è¨Šè¼¸å…¥å›èª¿å‡½æ•¸"""
        if status:
            print(f"âš ï¸  éŸ³è¨Šç‹€æ…‹: {status}")
        
        if self.is_recording:
            self.buffer.add(indata)
            
            # æª¢æ¸¬éœéŸ³
            energy = np.mean(np.abs(indata))
            
            if energy < Config.SILENCE_THRESHOLD:
                if self.silence_start is None:
                    self.silence_start = time.time()
                elif time.time() - self.silence_start > Config.SILENCE_DURATION:
                    print("ğŸ”‡ æª¢æ¸¬åˆ°æŒçºŒéœéŸ³")
                    self.stop()
            else:
                self.silence_start = None
    
    def start(self):
        """é–‹å§‹éŒ„éŸ³"""
        if self.is_recording:
            print("âš ï¸  å·²åœ¨éŒ„éŸ³ä¸­")
            return
        
        print("ğŸ¤ é–‹å§‹éŒ„éŸ³...")
        self.is_recording = True
        self.buffer.clear()
        self.silence_start = None
        
        try:
            self.stream = sd.InputStream(
                callback=self._callback,
                channels=Config.CHANNELS,
                samplerate=Config.SAMPLE_RATE,
                dtype=Config.DTYPE
            )
            self.stream.start()
            
        except Exception as e:
            print(f"âŒ éŒ„éŸ³å•Ÿå‹•å¤±æ•—: {e}")
            self.is_recording = False
    
    def stop(self):
        """åœæ­¢éŒ„éŸ³"""
        if not self.is_recording:
            return
        
        self.is_recording = False
        
        if self.stream:
            self.stream.stop()
            self.stream.close()
            self.stream = None
        
        duration = self.buffer.get_duration()
        print(f"â¹ï¸  éŒ„éŸ³åœæ­¢ï¼ˆæ™‚é•·: {duration:.2f} ç§’ï¼‰")
    
    def is_active(self):
        """æª¢æŸ¥æ˜¯å¦æ­£åœ¨éŒ„éŸ³"""
        return self.is_recording


# ==================== ä¸»ç³»çµ± ====================
class VoiceAssistantSTT:
    """èªéŸ³åŠ©ç† STT ç³»çµ±"""
    
    def __init__(self):
        """åˆå§‹åŒ–ç³»çµ±"""
        print("\n" + "="*60)
        print("æ¡ŒéŠèªéŸ³åŠ©ç† - STT ç³»çµ± (faster-whisper)")
        print("="*60)
        
        # å»ºç«‹æ—¥èªŒç›®éŒ„
        Config.LOG_DIR.mkdir(exist_ok=True)
        
        # åˆå§‹åŒ–çµ„ä»¶
        self.buffer = AudioBuffer(
            Config.MAX_RECORDING_DURATION,
            Config.SAMPLE_RATE
        )
        self.recorder = AudioRecorder(self.buffer)
        self.vad = VADProcessor(
            Config.SAMPLE_RATE,
            Config.VAD_THRESHOLD
        )
        self.stt = FasterWhisperSTT(
            Config.WHISPER_MODEL,
            Config.WHISPER_DEVICE,
            Config.WHISPER_COMPUTE_TYPE,
            Config.WHISPER_LANGUAGE
        )
        
        Config.print_config()
    
    def process_audio(self):
        """è™•ç†éŒ„éŸ³çš„éŸ³è¨Š"""
        print("\n" + "-"*60)
        print("âš™ï¸  è™•ç†éŸ³è¨Š...")
        
        # 1. ç²å–éŸ³è¨Šæ•¸æ“šï¼ˆfloat32 æ ¼å¼ï¼‰
        audio_float32 = self.buffer.get_array()
        duration = self.buffer.get_duration()
        
        if len(audio_float32) == 0:
            print("âŒ ç„¡éŸ³è¨Šæ•¸æ“š")
            return None
        
        print(f"ğŸ“Š éŸ³è¨Šæ™‚é•·: {duration:.2f} ç§’")
        print(f"ğŸ“Š æ¨£æœ¬æ•¸é‡: {len(audio_float32)}")
        
        # 2. VAD æª¢æ¸¬
        print("ğŸ” VAD æª¢æ¸¬ä¸­...")
        has_speech = self.vad.has_speech(audio_float32)
        
        if not has_speech:
            print("âŒ æœªæª¢æ¸¬åˆ°æœ‰æ•ˆèªéŸ³")
            return None
        
        print("âœ… æª¢æ¸¬åˆ°èªéŸ³")
        
        # 3. èªéŸ³è½‰æ–‡å­—
        print("ğŸ—£ï¸  èªéŸ³è½‰æ–‡å­—ä¸­...")
        text = self.stt.transcribe(audio_float32)
        
        if not text:
            print("âŒ è½‰éŒ„å¤±æ•—æˆ–ç„¡å…§å®¹")
            return None
        
        print(f"âœ… è¾¨è­˜çµæœ: {text}")
        
        # 4. é™¤éŒ¯æ¨¡å¼ï¼šä¿å­˜éŸ³è¨Š
        if Config.DEBUG_MODE:
            self._save_debug_audio(audio_float32, text)
        
        print("-"*60 + "\n")
        return text
    
    def _save_debug_audio(self, audio_float32, text):
        """ä¿å­˜é™¤éŒ¯éŸ³è¨Šå’Œæ–‡å­—"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # è½‰å› int16 ä¿å­˜
        audio_int16 = (audio_float32 * 32768.0).astype(np.int16)
        
        # ä¿å­˜éŸ³è¨Š
        audio_file = Config.LOG_DIR / f"audio_{timestamp}.wav"
        wavfile.write(audio_file, Config.SAMPLE_RATE, audio_int16)
        
        # ä¿å­˜æ–‡å­—
        text_file = Config.LOG_DIR / f"text_{timestamp}.txt"
        with open(text_file, 'w', encoding='utf-8') as f:
            f.write(text)
        
        print(f"ğŸ’¾ å·²ä¿å­˜: {audio_file.name} & {text_file.name}")
    
    def run_interactive(self):
        """é‹è¡Œäº’å‹•æ¨¡å¼"""
        print("ğŸ® äº’å‹•æ¨¡å¼å•Ÿå‹•")
        print("-"*60)
        print("æŒ‡ä»¤:")
        print("  Enter      - é–‹å§‹/åœæ­¢éŒ„éŸ³")
        print("  q + Enter  - é€€å‡º")
        print("-"*60)
        
        try:
            while True:
                cmd = input("\nğŸ‘‰ ").strip().lower()
                
                if cmd == 'q':
                    print("ğŸ‘‹ å†è¦‹ï¼")
                    break
                
                if not self.recorder.is_active():
                    self.recorder.start()
                    print("ğŸ’¡ æŒ‰ Enter åœæ­¢éŒ„éŸ³")
                else:
                    self.recorder.stop()
                    self.process_audio()
        
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ ç¨‹å¼ä¸­æ–·")
        
        finally:
            if self.recorder.is_active():
                self.recorder.stop()
    
    def run_button_mode(self):
        """é‹è¡ŒæŒ‰éˆ•æ¨¡å¼ï¼ˆæ¨¡æ“¬ï¼‰"""
        print("ğŸ”˜ æŒ‰éˆ•æ¨¡å¼å•Ÿå‹•")
        print("-"*60)
        print("æŒ‰ Enter æ¨¡æ“¬æŒ‰ä¸‹æŒ‰éˆ•ï¼ˆé–‹å§‹éŒ„éŸ³ï¼‰")
        print("éŒ„éŸ³å°‡åœ¨éœéŸ³æˆ–è¶…æ™‚å¾Œè‡ªå‹•åœæ­¢")
        print("è¼¸å…¥ 'q' é€€å‡º")
        print("-"*60)
        
        try:
            while True:
                cmd = input("\nğŸ‘‰ æŒ‰éˆ•: ").strip().lower()
                
                if cmd == 'q':
                    print("ğŸ‘‹ å†è¦‹ï¼")
                    break
                
                self.recorder.start()
                
                start = time.time()
                while self.recorder.is_active():
                    time.sleep(0.1)
                    if time.time() - start > Config.MAX_RECORDING_DURATION:
                        print("â±ï¸  é”åˆ°æœ€å¤§éŒ„éŸ³æ™‚é•·")
                        self.recorder.stop()
                        break
                
                self.process_audio()
        
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ ç¨‹å¼ä¸­æ–·")
        
        finally:
            if self.recorder.is_active():
                self.recorder.stop()


# ==================== ä¸»ç¨‹å¼å…¥å£ ====================
def main():
    """ä¸»ç¨‹å¼"""
    assistant = VoiceAssistantSTT()
    
    print("é¸æ“‡é‹è¡Œæ¨¡å¼:")
    print("  1 - äº’å‹•æ¨¡å¼ï¼ˆæ‰‹å‹•æ§åˆ¶éŒ„éŸ³ï¼‰")
    print("  2 - æŒ‰éˆ•æ¨¡å¼ï¼ˆè‡ªå‹•åœæ­¢ï¼‰")
    
    choice = input("\nè«‹é¸æ“‡ (1/2): ").strip()
    
    if choice == '2':
        assistant.run_button_mode()
    else:
        assistant.run_interactive()


if __name__ == "__main__":
    main()