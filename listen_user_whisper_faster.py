"""
æ¡ŒéŠèªéŸ³åŠ©ç† - å®Œæ•´ VAD ç³»çµ±ï¼ˆæ•´åˆç‰ˆï¼‰

æ§åˆ¶æ–¹å¼ï¼š
1. Enter é–‹å§‹éŒ„éŸ³
2. Enter åœæ­¢éŒ„éŸ³ æˆ– 40 ç§’è‡ªå‹•åœæ­¢

è™•ç†æµç¨‹ï¼š
- Callback: RNNoise é™å™ª â†’ WebRTC VAD â†’ ä¿å­˜èªéŸ³æ®µ
- è™•ç†: é™å™ªèªéŸ³æ®µ â†’ å¾Œç½®é©—è­‰ â†’ Whisper è½‰éŒ„
"""

import sounddevice as sd
import numpy as np
import time
import threading
from datetime import datetime
from collections import deque
from pathlib import Path
import gemini_flash_test
import edgTTS
import asyncio
# æª¢æŸ¥ä¾è³´
try:
    from faster_whisper import WhisperModel
except ImportError:
    print("âŒ è«‹å®‰è£: pip install faster-whisper")
    exit(1)

try:
    import webrtcvad
except ImportError:
    print("âŒ è«‹å®‰è£: pip install webrtcvad")
    exit(1)

try:
    from scipy.io import wavfile
    from scipy import signal
except ImportError:
    print("âŒ è«‹å®‰è£: pip install scipy")
    exit(1)

# RNNoiseï¼ˆå¯é¸ï¼‰
try:
    from pyrnnoise import RNNoise
    RNNOISE_AVAILABLE = True
except ImportError:
    RNNOISE_AVAILABLE = False
    print("âš ï¸  RNNoise æœªå®‰è£ï¼ˆå¯é¸ï¼‰ï¼Œé™å™ªåŠŸèƒ½ä¸å¯ç”¨")


# ==================== é…ç½®é¡ ====================
class Config:
    """ç³»çµ±é…ç½®"""
    # éŸ³è¨Šåƒæ•¸
    SAMPLE_RATE = 16000
    CHANNELS = 1
    DTYPE = 'int16'
    FRAME_DURATION_MS = 20
    
    # éŒ„éŸ³æ§åˆ¶
    MAX_RECORDING_DURATION = 40  # æœ€å¤§ 40 ç§’
    
    # é™å™ª
    ENABLE_RNNOISE = True
    RNNOISE_IN_CALLBACK = True
    
    # VAD
    VAD_STRATEGY = "webrtc_only"
    WEBRTC_AGGRESSIVENESS = 1
    
    # ç‹€æ…‹æ©Ÿ
    SPEECH_START_FRAMES = 3
    SPEECH_END_FRAMES = 25
    PRE_SPEECH_FRAMES = 10
    POST_SPEECH_FRAMES = 5
    
    # å¾Œç½®é©—è­‰
    MIN_SPEECH_DURATION = 0.5
    MAX_SPEECH_DURATION = 60
    MIN_ENERGY_THRESHOLD = 50
    
    # Whisper
    WHISPER_MODEL = "small" 
    WHISPER_DEVICE = "cpu"
    WHISPER_COMPUTE_TYPE = "int8"
    WHISPER_LANGUAGE = "zh"
    
    # ç³»çµ±
    DEBUG_MODE = True
    LOG_DIR = Path("logs")
    
    @classmethod
    def print_config(cls):
        """åˆ—å°é…ç½®"""
        print("\n" + "="*60)
        print("ç³»çµ±é…ç½®")
        print("="*60)
        print(f"æœ€å¤§éŒ„éŸ³æ™‚é•·: {cls.MAX_RECORDING_DURATION} ç§’")
        print(f"RNNoise é™å™ª: {'é–‹å•Ÿï¼ˆå¯¦æ™‚ï¼‰' if cls.ENABLE_RNNOISE and RNNOISE_AVAILABLE else 'é—œé–‰'}")
        print(f"VAD ç­–ç•¥: WebRTC (æ¿€é€²åº¦={cls.WEBRTC_AGGRESSIVENESS})")
        print(f"Whisper æ¨¡å‹: {cls.WHISPER_MODEL}")
        print(f"é™¤éŒ¯æ¨¡å¼: {'é–‹å•Ÿ' if cls.DEBUG_MODE else 'é—œé–‰'}")
        print("="*60 + "\n")


# ==================== RNNoise é™å™ª ====================
class RNNoiseProcessor:
    """RNNoise é™å™ªè™•ç†å™¨"""
    
    def __init__(self, sample_rate=16000):
        self.sample_rate = sample_rate
        self.denoiser = None
        self.available = RNNOISE_AVAILABLE
        
        if not self.available:
            return
        
        try:
            self.denoiser = RNNoise(sample_rate=sample_rate)
            print("âœ… RNNoise å·²åˆå§‹åŒ–")
        except Exception as e:
            print(f"âš ï¸  RNNoise åˆå§‹åŒ–å¤±æ•—: {e}")
            self.available = False
    
    def process_frame(self, audio_int16):
        """è™•ç†éŸ³è¨Šå¹€"""
        if not self.available or self.denoiser is None:
            return audio_int16
        
        try:
            # é‡æ¡æ¨£åˆ° 48kHz
            audio_48k = self._resample_to_48k(audio_int16)
            
            # è½‰ float32
            audio_float = audio_48k.astype(np.float32) / 32768.0
            
            # é™å™ª
            denoised_float = self.denoiser.denoise_frame(audio_float)
            
            # è½‰å› int16
            audio_denoised_48k = (denoised_float * 32768.0).astype(np.int16)
            
            # é‡æ¡æ¨£å› 16kHz
            audio_denoised_16k = self._resample_to_16k(audio_denoised_48k)
            
            return audio_denoised_16k
            
        except Exception as e:
            return audio_int16
    
    def _resample_to_48k(self, audio_16k):
        """16kHz â†’ 48kHz"""
        num_samples = int(len(audio_16k) * 48000 / 16000)
        return signal.resample(audio_16k, num_samples).astype(np.int16)
    
    def _resample_to_16k(self, audio_48k):
        """48kHz â†’ 16kHz"""
        num_samples = int(len(audio_48k) * 16000 / 48000)
        return signal.resample(audio_48k, num_samples).astype(np.int16)
    
    def reset(self):
        """é‡ç½®ç‹€æ…‹"""
        if self.available and self.denoiser is not None:
            try:
                self.denoiser = RNNoise(sample_rate=self.sample_rate)
            except:
                pass


# ==================== WebRTC VAD ====================
class WebRTCVAD:
    """WebRTC VAD è™•ç†å™¨"""
    
    def __init__(self, sample_rate=16000, aggressiveness=1):
        self.sample_rate = sample_rate
        self.vad = webrtcvad.Vad()
        self.vad.set_mode(aggressiveness)
        print(f"âœ… WebRTC VAD å·²åˆå§‹åŒ–ï¼ˆæ¿€é€²åº¦: {aggressiveness}ï¼‰")
    
    def is_speech(self, audio_int16):
        """åˆ¤æ–·æ˜¯å¦ç‚ºèªéŸ³"""
        try:
            frame_length = int(self.sample_rate * Config.FRAME_DURATION_MS / 1000)
            
            if len(audio_int16) != frame_length:
                audio_int16 = self._adjust_frame_size(audio_int16, frame_length)
            
            is_speech = self.vad.is_speech(
                audio_int16.tobytes(),
                sample_rate=self.sample_rate
            )
            
            return is_speech
            
        except Exception as e:
            # é™ç´šåˆ°èƒ½é‡æª¢æ¸¬
            energy = np.mean(np.abs(audio_int16))
            return energy > Config.MIN_ENERGY_THRESHOLD
    
    def _adjust_frame_size(self, audio, target_length):
        """èª¿æ•´å¹€å¤§å°"""
        if len(audio) < target_length:
            return np.pad(audio, (0, target_length - len(audio)), mode='constant')
        else:
            return audio[:target_length]


# ==================== éŸ³è¨Šç·©è¡å€ ====================
class AudioBuffer:
    """éŸ³è¨Šç·©è¡å€"""
    
    def __init__(self):
        self.buffer = []
        self.lock = threading.Lock()
    
    def add(self, data):
        """æ·»åŠ æ•¸æ“š"""
        with self.lock:
            self.buffer.append(data)
    
    def get_array(self):
        """ç²å–å®Œæ•´éŸ³è¨Š"""
        with self.lock:
            if not self.buffer:
                return np.array([], dtype='int16')
            return np.concatenate(self.buffer)
    
    def clear(self):
        """æ¸…ç©º"""
        with self.lock:
            self.buffer.clear()
    
    def get_duration(self):
        """ç²å–æ™‚é•·"""
        audio = self.get_array()
        return len(audio) / Config.SAMPLE_RATE


# ==================== æ™ºèƒ½éŒ„éŸ³å™¨ ====================
class SmartAudioRecorder:
    """æ™ºèƒ½éŒ„éŸ³å™¨ï¼ˆRNNoise + WebRTC VAD + Enteræ§åˆ¶ + è¶…æ™‚ï¼‰"""
    
    def __init__(self):
        """åˆå§‹åŒ–"""
        # é™å™ªå™¨
        if Config.ENABLE_RNNOISE and Config.RNNOISE_IN_CALLBACK:
            self.denoiser = RNNoiseProcessor(Config.SAMPLE_RATE)
        else:
            self.denoiser = None
        
        # VAD
        self.vad = WebRTCVAD(
            Config.SAMPLE_RATE,
            Config.WEBRTC_AGGRESSIVENESS
        )
        
        # ç·©è¡å€
        self.speech_buffer = AudioBuffer()
        self.pre_buffer = deque(maxlen=Config.PRE_SPEECH_FRAMES)
        
        # ç‹€æ…‹
        self.is_recording = False
        self.is_speaking = False
        self.consecutive_speech = 0
        self.consecutive_silence = 0
        self.stream = None
        self.start_time = None
        
        # âœ… è¶…æ™‚æ§åˆ¶
        self.stop_event = threading.Event()
        self.monitor_thread = None
        
        # çµ±è¨ˆ
        self.total_frames = 0
        self.speech_frames = 0
    
    def _callback(self, indata, frames, time_info, status):
        """éŸ³è¨Šå›èª¿"""
        if status:
            print(f"âš ï¸  {status}")
        
        if not self.is_recording:
            return
        
        self.total_frames += 1
        
        # è½‰ int16
        audio_frame = indata.flatten().astype('int16')
        
        # é™å™ª
        if self.denoiser:
            audio_frame = self.denoiser.process_frame(audio_frame)
        
        # VAD åˆ¤æ–·
        is_speech = self.vad.is_speech(audio_frame)
        
        # ç‹€æ…‹æ©Ÿ
        self._handle_speech_state(audio_frame, is_speech)
    
    def _handle_speech_state(self, audio_frame, is_speech):
        """ç‹€æ…‹æ©Ÿè™•ç†"""
        
        if is_speech:
            self.consecutive_speech += 1
            self.consecutive_silence = 0
            self.speech_frames += 1
            
            # èªéŸ³é–‹å§‹
            if not self.is_speaking:
                if self.consecutive_speech >= Config.SPEECH_START_FRAMES:
                    self.is_speaking = True
                    print(f"ğŸ—£ï¸  èªéŸ³é–‹å§‹ï¼ˆå¹€ {self.total_frames}ï¼‰")
                    
                    # åŠ å…¥å‰å°ç·©è¡
                    for pre_frame in self.pre_buffer:
                        self.speech_buffer.add(pre_frame)
                    
                    self.pre_buffer.clear()
            
            # ä¿å­˜èªéŸ³
            self.speech_buffer.add(audio_frame)
            self.pre_buffer.append(audio_frame)
        
        else:  # éèªéŸ³
            self.consecutive_silence += 1
            self.consecutive_speech = 0
            
            # èªéŸ³å¯èƒ½çµæŸ
            if self.is_speaking:
                # ä¿å­˜å¾Œå°
                if self.consecutive_silence <= Config.POST_SPEECH_FRAMES:
                    self.speech_buffer.add(audio_frame)
                
                # ç¢ºèªçµæŸ
                elif self.consecutive_silence >= Config.SPEECH_END_FRAMES:
                    self.is_speaking = False
                    duration = self.speech_buffer.get_duration()
                    print(f"ğŸ”‡ èªéŸ³çµæŸï¼ˆæ™‚é•·: {duration:.2f}sï¼‰")
                    
                    # âœ… èªéŸ³çµæŸå¾Œè‡ªå‹•åœæ­¢
                    self.stop()
            
            else:
                # ç¶­è­·å‰å°ç·©è¡
                self.pre_buffer.append(audio_frame)
    
    def _monitor_timeout(self):
        """ç›£æ§è¶…æ™‚ï¼ˆç¨ç«‹ç·šç¨‹ï¼‰"""
        # âœ… ç­‰å¾…åœæ­¢äº‹ä»¶æˆ–è¶…æ™‚
        timeout_reached = not self.stop_event.wait(
            timeout=Config.MAX_RECORDING_DURATION
        )
        
        if timeout_reached and self.is_recording:
            print(f"\nâ±ï¸  é”åˆ° {Config.MAX_RECORDING_DURATION} ç§’ï¼Œè‡ªå‹•åœæ­¢")
            self.stop()
    
    def start(self):
        """é–‹å§‹éŒ„éŸ³"""
        if self.is_recording:
            print("âš ï¸  å·²åœ¨éŒ„éŸ³ä¸­")
            return
        
        print(f"ğŸ¤ é–‹å§‹éŒ„éŸ³ï¼ˆæœ€é•· {Config.MAX_RECORDING_DURATION} ç§’ï¼‰")
        
        # é‡ç½®ç‹€æ…‹
        self.is_recording = True
        self.is_speaking = False
        self.consecutive_speech = 0
        self.consecutive_silence = 0
        self.total_frames = 0
        self.speech_frames = 0
        self.start_time = time.time()
        self.stop_event.clear()
        
        # æ¸…ç©ºç·©è¡
        self.speech_buffer.clear()
        self.pre_buffer.clear()
        
        # é‡ç½®é™å™ªå™¨
        if self.denoiser:
            self.denoiser.reset()
        
        # é–‹å•ŸéŸ³è¨Šæµ
        try:
            frame_length = int(Config.SAMPLE_RATE * Config.FRAME_DURATION_MS / 1000)
            
            self.stream = sd.InputStream(
                callback=self._callback,
                channels=Config.CHANNELS,
                samplerate=Config.SAMPLE_RATE,
                dtype=Config.DTYPE,
                blocksize=frame_length
            )
            self.stream.start()
            
            # âœ… å•Ÿå‹•ç›£æ§ç·šç¨‹
            self.monitor_thread = threading.Thread(target=self._monitor_timeout)
            self.monitor_thread.daemon = True
            self.monitor_thread.start()
            
        except Exception as e:
            print(f"âŒ éŒ„éŸ³å•Ÿå‹•å¤±æ•—: {e}")
            self.is_recording = False
    
    def stop(self):
        """åœæ­¢éŒ„éŸ³"""
        if not self.is_recording:
            return
        
        self.is_recording = False
        self.stop_event.set()  # âœ… è§¸ç™¼åœæ­¢äº‹ä»¶
        
        # åœæ­¢éŸ³è¨Šæµ
        if self.stream:
            self.stream.stop()
            self.stream.close()
            self.stream = None
        
        # ç­‰å¾…ç›£æ§ç·šç¨‹
        if self.monitor_thread and self.monitor_thread.is_alive():
            self.monitor_thread.join(timeout=1)
        
        duration = time.time() - self.start_time
        speech_duration = self.speech_buffer.get_duration()
        
        print(f"â¹ï¸  éŒ„éŸ³åœæ­¢")
        print(f"   ç¸½æ™‚é•·: {duration:.1f}s")
        print(f"   èªéŸ³æ®µ: {speech_duration:.1f}s")
        if self.total_frames > 0:
            print(f"   èªéŸ³å¹€ç‡: {self.speech_frames/self.total_frames*100:.1f}%")
    
    def get_speech(self):
        """ç²å–èªéŸ³æ®µ"""
        return self.speech_buffer.get_array()
    
    def is_active(self):
        """æ˜¯å¦æ­£åœ¨éŒ„éŸ³"""
        return self.is_recording


# ==================== Whisper STT ====================
class FasterWhisperSTT:
    """faster-whisper èªéŸ³è½‰æ–‡å­—"""
    
    def __init__(self, model_size="base", device="cpu", compute_type="int8", language="zh"):
        self.language = language
        self.model = None
        self._load_model(model_size, device, compute_type)
    
    def _load_model(self, model_size, device, compute_type):
        """è¼‰å…¥æ¨¡å‹"""
        try:
            print(f"â³ è¼‰å…¥ Whisper æ¨¡å‹: {model_size}")
            
            self.model = WhisperModel(
                model_size,
                device=device,
                compute_type=compute_type
            )
            
            print("âœ… Whisper å·²è¼‰å…¥")
            
        except Exception as e:
            print(f"âŒ Whisper è¼‰å…¥å¤±æ•—: {e}")
            self.model = None
    
    def transcribe(self, audio_int16):
        """è½‰éŒ„éŸ³è¨Š"""
        if self.model is None:
            return ""
        
        try:
            # è½‰ float32
            audio_float32 = audio_int16.astype(np.float32) / 32768.0
            
            # è½‰éŒ„
            start_time = time.time()
            segments, info = self.model.transcribe(
                audio_float32,
                language=self.language,
                beam_size=5,
                vad_filter=False,
                without_timestamps=True
            )
            
            text = "".join([segment.text for segment in segments]).strip()
            
            elapsed = time.time() - start_time
            print(f"â±ï¸  è½‰éŒ„è€—æ™‚: {elapsed:.2f} ç§’")
            
            return text
            
        except Exception as e:
            print(f"âŒ è½‰éŒ„éŒ¯èª¤: {e}")
            return ""


# ==================== ä¸»ç³»çµ± ====================
class VoiceAssistant:
    """èªéŸ³åŠ©ç†ä¸»ç³»çµ±"""
    
    def __init__(self):
        """åˆå§‹åŒ–"""
        print("\n" + "="*60)
        print("æ¡ŒéŠèªéŸ³åŠ©ç† - å®Œæ•´ç³»çµ±")
        print("="*60)
        
        # å»ºç«‹æ—¥èªŒç›®éŒ„
        Config.LOG_DIR.mkdir(exist_ok=True)
        
        # åˆå§‹åŒ–çµ„ä»¶
        self.recorder = SmartAudioRecorder()
        self.stt = FasterWhisperSTT(
            Config.WHISPER_MODEL,
            Config.WHISPER_DEVICE,
            Config.WHISPER_COMPUTE_TYPE,
            Config.WHISPER_LANGUAGE
        )
        
        Config.print_config()
    
    def process_speech(self):
        """è™•ç†èªéŸ³"""
        print("\n" + "-"*60)
        print("âš™ï¸  è™•ç†èªéŸ³...")
        
        # ç²å–èªéŸ³æ®µ
        audio_int16 = self.recorder.get_speech()
        
        if len(audio_int16) == 0:
            print("âŒ ç„¡èªéŸ³æ•¸æ“š")
            return None
        
        duration = len(audio_int16) / Config.SAMPLE_RATE
        print(f"ğŸ“Š èªéŸ³æ™‚é•·: {duration:.2f} ç§’")
        
        # å¾Œç½®é©—è­‰
        if not self._validate_speech(audio_int16):
            print("âŒ èªéŸ³é©—è­‰å¤±æ•—")
            return None
        
        # è½‰éŒ„
        print("ğŸ—£ï¸  èªéŸ³è½‰æ–‡å­—ä¸­...")
        text = self.stt.transcribe(audio_int16)
        
        if not text:
            print("âŒ è½‰éŒ„å¤±æ•—æˆ–ç„¡å…§å®¹")
            return None
        
        print(f"âœ… è¾¨è­˜çµæœ: {text}")
        
        # é™¤éŒ¯æ¨¡å¼
        if Config.DEBUG_MODE:
            self._save_debug_audio(audio_int16, text)
        
        print("-"*60 + "\n")
        return text
    
    def _validate_speech(self, audio_int16):
        """å¾Œç½®é©—è­‰"""
        duration = len(audio_int16) / Config.SAMPLE_RATE
        
        # æ™‚é•·æª¢æŸ¥
        if duration < Config.MIN_SPEECH_DURATION:
            print(f"  âš ï¸  èªéŸ³å¤ªçŸ­: {duration:.2f}s")
            return False
        
        if duration > Config.MAX_SPEECH_DURATION:
            print(f"  âš ï¸  èªéŸ³å¤ªé•·: {duration:.2f}s")
            return False
        
        # èƒ½é‡æª¢æŸ¥
        energy = np.mean(np.abs(audio_int16))
        if energy < Config.MIN_ENERGY_THRESHOLD:
            print(f"  âš ï¸  èƒ½é‡å¤ªä½: {energy:.1f}")
            return False
        
        print(f"âœ… é©—è­‰é€šéï¼ˆæ™‚é•·: {duration:.2f}s, èƒ½é‡: {energy:.1f}ï¼‰")
        return True
    
    def _save_debug_audio(self, audio_int16, text):
        """ä¿å­˜é™¤éŒ¯éŸ³è¨Š"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # ä¿å­˜éŸ³è¨Š
        audio_file = Config.LOG_DIR / f"speech_{timestamp}.wav"
        wavfile.write(audio_file, Config.SAMPLE_RATE, audio_int16)
        
        # ä¿å­˜æ–‡å­—
        text_file = Config.LOG_DIR / f"text_{timestamp}.txt"
        with open(text_file, 'w', encoding='utf-8') as f:
            f.write(f"è¾¨è­˜çµæœ: {text}\n")
            f.write(f"æ™‚é•·: {len(audio_int16)/Config.SAMPLE_RATE:.2f}s\n")
            f.write(f"é™å™ª: {'é–‹å•Ÿ' if Config.ENABLE_RNNOISE else 'é—œé–‰'}\n")
            f.write(f"VAD: WebRTC (mode={Config.WEBRTC_AGGRESSIVENESS})\n")
        
        print(f"ğŸ’¾ å·²ä¿å­˜: {audio_file.name}")
    
    def run(self):
        """é‹è¡Œä¸»å¾ªç’°"""
        print("ğŸ® ç³»çµ±å•Ÿå‹•")
        print("-"*60)
        print("æ“ä½œèªªæ˜:")
        print("  1. æŒ‰ Enter é–‹å§‹éŒ„éŸ³")
        print("  2. æŒ‰ Enter åœæ­¢éŒ„éŸ³ï¼ˆæˆ–ç­‰å¾…èªéŸ³çµæŸ/è¶…æ™‚ï¼‰")
        print("  3. è¼¸å…¥ 'q' é€€å‡º")
        print("-"*60)
        
        try:
            while True:
                cmd = input("\nğŸ‘‰ ").strip().lower()
                
                if cmd == 'q':
                    print("ğŸ‘‹ å†è¦‹ï¼")
                    break
                
                # é–‹å§‹éŒ„éŸ³
                self.recorder.start()
                
                # âœ… ç­‰å¾…åœæ­¢ï¼ˆEnter æˆ–è‡ªå‹•ï¼‰
                print("ğŸ“ éŒ„éŸ³ä¸­... æŒ‰ Enter åœæ­¢")
                input()
                
                # åœæ­¢éŒ„éŸ³
                self.recorder.stop()
                
                # è™•ç†èªéŸ³
                userAsk=self.process_speech()
                reply_text=""
                if userAsk:
                    reply_text = gemini_flash_test.try_cloud_LLM(userAsk)
                if reply_text:
                    asyncio.run(edgTTS.play_audio_stream(reply_text))
                    time.sleep(3)
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ ç¨‹å¼ä¸­æ–·")
        
        finally:
            if self.recorder.is_active():
                self.recorder.stop()


# ==================== ä¸»ç¨‹å¼ ====================
def main():
    """ä¸»ç¨‹å¼"""
    assistant = VoiceAssistant()
    assistant.run()


if __name__ == "__main__":
    main()